services:
  # --- Storage ---
  namenode:
    image: apache/hadoop:3.4.1
    entrypoint: /bin/bash
    command: -c "sed 's/\r$//' /init/start_namenode.sh > /tmp/start_nn.sh && bash /tmp/start_nn.sh"
    ports: [ "9870:9870", "9000:9000" ]
    environment:
      ENSURE_NAMENO_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./scripts/init/start_namenode.sh:/init/start_namenode.sh:ro
      - ./data/Tweets.csv:/local_data/Tweets.csv:ro
    healthcheck:
      test: [ "CMD-SHELL", "hdfs dfsadmin -safemode get 2>/dev/null | grep -q 'OFF' || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  datanode:
    image: apache/hadoop:3.4.1
    command: [ "hdfs", "datanode" ]
    depends_on:
      namenode:
        condition: service_healthy
    volumes:
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:9864/jmx || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # --- Metadata ---
  postgres-db:
    image: postgres:latest
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive_password
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U hive -d metastore" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s

  hive-metastore:
    build: ./docker/hive
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-db:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive_password"
    ports: [ "9083:9083" ]
    volumes:
      - ./conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./conf/core-site.xml:/opt/hive/conf/core-site.xml:ro
    depends_on:
      postgres-db:
        condition: service_healthy
      namenode:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9083' || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s

  hiveserver2:
    build: ./docker/hive
    entrypoint: /bin/bash
    command: -c "sed 's/\\r$//' /init/start_hiveserver2.sh > /tmp/start_hs2.sh && bash /tmp/start_hs2.sh"
    environment:
      SERVICE_NAME: hiveserver2
    ports: [ "10000:10000" ]
    volumes:
      - ./conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./conf/core-site.xml:/opt/hive/conf/core-site.xml:ro
      - ./scripts/init/start_hiveserver2.sh:/init/start_hiveserver2.sh:ro
      - ./scripts/sql/hive_tables.sql:/sql/hive_tables.sql:ro
    depends_on:
      hive-metastore:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "beeline -u 'jdbc:hive2://localhost:10000/default' -n hive -e 'SHOW DATABASES;' || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 120s

  # --- Streaming ---
  kafka:
    image: apache/kafka:latest
    ports: [ "9092:9092" ]
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    command: >
      bash -c "/opt/kafka/bin/kafka-storage.sh format --standalone -t $(/opt/kafka/bin/kafka-storage.sh random-uuid) -c /opt/kafka/config/server.properties && /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties --override advertised.listeners=PLAINTEXT://kafka:9092"
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  flink-jobmanager:
    build: ./docker/flink
    ports: [ "8081:8081" ]
    command: jobmanager
    environment:
      FLINK_PROPERTIES: "jobmanager.rpc.address: flink-jobmanager"
    volumes:
      - ./scripts/jobs/stream_job.py:/jobs/stream_job.py:ro
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:8081/overview || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  flink-taskmanager:
    build: ./docker/flink
    command: taskmanager
    environment:
      FLINK_PROPERTIES: "jobmanager.rpc.address: flink-jobmanager

        taskmanager.numberOfTaskSlots: 2"
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "ps aux | grep java || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --- Batch ---
  spark-master:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    image: hpdpproject-spark:custom
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports: [ "8080:8080", "7077:7077" ]
    volumes:
      - ./conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
      - ./scripts/jobs/batch_job.py:/jobs/batch_job.py:ro
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:8080 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  spark-worker:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    image: hpdpproject-spark:custom
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./conf/hive-site.xml:/opt/spark/conf/hive-site.xml:ro
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:8081 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # --- Tools ---
  client:
    image: python:3.9-slim
    volumes:
      - ./scripts/jobs/producer.py:/jobs/producer.py:ro
      - ./data/Tweets.csv:/local_data/Tweets.csv:ro
    command: tail -f /dev/null
